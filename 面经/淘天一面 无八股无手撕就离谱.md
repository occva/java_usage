以下是对这些面试问题的详细解答，结合你的简历内容和常见技术场景，帮助你更好地准备面试：
[淘天一面 无八股无手撕就离谱_牛客网](https://www.nowcoder.com/feed/main/detail/203bedff7efe48c1a6f2019e146abaed?sourceSSR=home)
### 数据库模糊匹配与分词
1. **在用ES之前，你的数据库模糊匹配怎么做的？**
   - 在没有使用Elasticsearch之前，我主要通过SQL语句中的`LIKE`操作符实现模糊匹配。例如，使用`SELECT * FROM table WHERE column LIKE '%keyword%'`来查找包含特定关键字的数据。这种方法简单直接，但对于复杂的模糊匹配和海量数据的检索效率较低，且对大小写、字符组合等不够灵活。

2. **怎么做的分词？**
   - 在项目中，我使用了开源的中文分词工具**jieba**。它支持多种分词模式（如精确模式、全模式、搜索引擎模式），可以根据具体需求选择合适的模式。例如，在处理用户输入的搜索关键词时，使用jieba的搜索引擎模式进行分词，将长句子拆分成多个关键词，然后通过这些关键词进行更精准的搜索匹配。

3. **为什么选择用分词器，我看你简历上另一个项目也写了nltk，考虑过其它NLP工具吗？**
   - 选择分词器主要是为了处理中文文本的特殊性。中文文本没有明显的单词分隔符，分词是进行文本处理和信息检索的基础步骤。在另一个项目中使用NLTK主要是因为它提供了丰富的英文文本处理功能，如词性标注、句法分析等。对于中文处理，jieba更适合。我也考虑过其他NLP工具，如HanLP，它功能更强大，但jieba的性能和易用性更适合项目需求。

### 地理检索与地图区块划分
4. **地理检索具体怎么做的？**
   - 在项目中，我们使用了**MySQL的地理空间扩展**功能。通过存储经纬度信息，并利用`ST_Distance_Sphere`等函数计算两点之间的距离，实现地理检索。例如，查询某个用户附近的商家时，可以根据用户的位置和商家的经纬度，筛选出一定距离范围内的商家。

5. **地图区块划分的颗粒度怎么把握？**
   - 地图区块划分的颗粒度需要根据应用场景和数据量来决定。如果数据量较大且查询精度要求不高，可以划分较粗的区块，如按城市或区域划分；如果需要更精确的检索，可以划分更细的区块，如按街道或小区划分。在实际项目中，我们通过分析用户查询的热点区域和数据分布，动态调整区块划分的颗粒度，以平衡查询效率和存储成本。

### 分布式系统与消息队列
6. **说说对雪花算法的了解？**
   - 雪花算法是一种分布式ID生成算法，主要用于在分布式系统中生成全局唯一且有序的ID。它将ID分为几部分：时间戳、工作机器ID、序列号。时间戳保证了ID的有序性，工作机器ID和序列号保证了在不同机器和同一时间戳下的唯一性。它适用于高并发场景下的ID生成需求，如数据库主键生成等。

7. **为什么用Kafka不用RabbitMQ？**
   - 在项目中选择Kafka而不是RabbitMQ，主要是因为Kafka在处理高吞吐量数据和分布式日志方面具有优势。Kafka支持高吞吐量的消息发布和订阅，适合处理大规模数据流。此外，Kafka的持久化存储和消息重放功能也更适合我们的业务需求，如日志分析和数据备份。

8. **有做过压测吗？模拟的并发量多少？**
   - 是的，我们对系统进行了压力测试。使用了**JMeter**工具，模拟了不同并发量下的系统性能。在黑马点评项目中，我们模拟了最高达**1000并发用户**进行秒杀操作，以测试系统的响应时间和资源利用率。通过压测，我们发现并优化了多个性能瓶颈，如数据库连接池的配置和缓存策略。

### 系统架构与性能优化
9. **为什么用HTTP不用RPC？**
   - 在项目中使用HTTP而不是RPC，主要是因为HTTP协议具有广泛的兼容性和易用性。HTTP协议简单明了，易于调试和监控，且大多数现代开发框架都支持HTTP接口。此外，HTTP协议的无状态特性也适合我们的分布式系统架构。虽然RPC在某些场景下性能更高，但HTTP的通用性和灵活性更适合我们的业务需求。

10. **怎么做的流量削峰？**
    - 为了应对流量高峰，我们采用了**限流和降级**策略。使用**Guava RateLimiter**对接口进行限流，限制单位时间内请求的数量。同时，通过**熔断机制**在系统负载过高时自动降级非核心功能，确保核心业务的稳定运行。此外，还通过**消息队列**将部分请求异步处理，缓解流量压力。

11. **怎么防止超卖和保证一致性？**
    - 在项目中，我们通过**分布式锁**和**乐观锁**机制防止超卖。使用Redisson实现分布式锁，在处理订单时确保同一商品在同一时间只能被一个用户操作。同时，在数据库层面使用乐观锁（通过版本号字段）确保数据一致性。在秒杀场景中，结合Redis的原子操作（如`INCR`）确保库存的正确扣减。

12. **如何保证秒杀先到先得，保证公平性？**
    - 为了保证秒杀的公平性，我们采用了**队列机制**。所有秒杀请求首先进入一个消息队列，系统按照队列的顺序依次处理请求。同时，通过**分布式锁**确保同一时间只有一个请求能够操作库存，确保先到先得。此外，我们还对系统进行了严格的性能优化，确保在高并发场景下响应时间尽可能短。

### 项目背景与技术选型
13. **怎么想到做这个项目的？**
    - 这个项目是基于市场需求和用户痛点提出的。我们发现市场上缺乏一个高效、便捷的本地消费与社交互动平台，用户在寻找优惠和分享体验时存在诸多不便。因此，我们决定开发黑马点评，旨在为用户提供一站式的本地消费服务和社交互动体验。

14. **介绍RAG项目**
    - RAG（Retrieval-Augmented Generation）是一种结合检索和生成的自然语言处理框架。它通过检索模块从大规模文档集合中提取相关信息，然后将这些信息作为上下文输入到生成模块中，生成更准确、更丰富的回答。在我们的项目中，RAG用于提升问答系统的性能，通过检索相关文档并结合生成模型，提供更精准的用户答案。

15. **BGE M3嵌入后的具体向量长度？**
    - BGE M3是一种基于BERT的嵌入模型，用于将文本嵌入到高维向量空间。在我们的项目中，BGE M3嵌入后的向量长度为**768维**。这个长度是BERT基础模型的标准输出维度，适用于多种NLP任务，如文本相似度计算和语义检索。

16. **数据集多大？**
    - 在RAG项目中，我们使用的数据集大小约为**100万条文档**。这些文档包括用户生成的内容、专业文章和常见问题解答等，用于支持检索模块的高效运行。

17. **chunk怎么做的？**
    - 在RAG项目中，我们将文档分割成**固定长度的chunk**，每个chunk的长度约为**100个单词**。这样做的目的是为了在检索阶段能够快速定位到与问题最相关的片段，同时减少计算开销。分割后的chunk会被嵌入到向量空间中，用于后续的检索和生成任务。

18. **具体的RAG链路？**
    - RAG的具体链路包括以下几个步骤：
      1. **检索阶段**：用户输入问题后，通过嵌入模型将问题嵌入到向量空间，然后在文档集合中检索与问题最相关的chunk。
      2. **生成阶段**：将检索到的chunk作为上下文输入到生成模型（如GPT）中，生成最终的回答。
      3. **后处理**：对生成的回答进行语法和逻辑检查，确保回答的准确性和流畅性。

19. **query改写的效果大概长啥样？举个例子**
    - Query改写是指对用户输入的问题进行语义扩展和改写，以提高检索的准确性和召回率。例如，用户输入的问题是“如何学习Java编程？”改写后的query可能是“学习Java编程的最佳方法”或“Java编程入门教程”。通过这种方式，可以更好地匹配文档中的相关内容。

20. **每一路的top-k设的多少？**
    - 在RAG检索阶段，每一路的top-k值通常设置为**5**。这意味着检索模块会返回与问题最相关的5个chunk，然后生成模块会结合这些chunk生成最终的回答。这个值可以根据具体需求和资源情况进行调整。

21. **怎么想到的用RRF？**
    - RRF（Reciprocal Rank Fusion）是一种融合多个检索结果的方法，用于提高检索的准确性和鲁棒性。在我们的项目中，我们使用RRF融合多个检索模型的结果，以减少单一模型的偏差。通过RRF，可以更好地综合不同模型的优势，提高检索的综合性能